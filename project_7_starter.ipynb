{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 7: Combine Signals for Enhanced Alpha\n",
    "## Instructions\n",
    "Each problem consists of a function to implement and instructions on how to implement the function.  The parts of the function that need to be implemented are marked with a `# TODO` comment. After implementing the function, run the cell to test it against the unit tests we've provided. For each problem, we provide one or more unit tests from our `project_tests` package. These unit tests won't tell you if your answer is correct, but will warn you of any major errors. Your code will be checked for the correct solution when you submit it to Udacity.\n",
    "\n",
    "## Packages\n",
    "When you implement the functions, you'll only need to you use the packages you've used in the classroom, like [Pandas](https://pandas.pydata.org/) and [Numpy](http://www.numpy.org/). These packages will be imported for you. We recommend you don't add any import statements, otherwise the grader might not be able to run your code.\n",
    "\n",
    "The other packages that we're importing are `project_helper` and `project_tests`. These are custom packages built to help you solve the problems.  The `project_helper` module contains utility functions and graph functions. The `project_tests` contains the unit tests for all the problems.\n",
    "\n",
    "### Install Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting alphalens==0.3.2 (from -r requirements.txt (line 1))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a5/dc/2f9cd107d0d4cf6223d37d81ddfbbdbf0d703d03669b83810fa6b97f32e5/alphalens-0.3.2.tar.gz (18.9MB)\n",
      "\u001b[K    100% |████████████████████████████████| 18.9MB 2.0MB/s eta 0:00:01   21% |██████▉                         | 4.0MB 28.4MB/s eta 0:00:01    36% |███████████▊                    | 6.9MB 32.2MB/s eta 0:00:01    90% |█████████████████████████████   | 17.1MB 32.1MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting graphviz==0.10.1 (from -r requirements.txt (line 2))\n",
      "  Downloading https://files.pythonhosted.org/packages/1f/e2/ef2581b5b86625657afd32030f90cf2717456c1d2b711ba074bf007c0f1a/graphviz-0.10.1-py2.py3-none-any.whl\n",
      "Collecting numpy==1.13.3 (from -r requirements.txt (line 3))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/57/a7/e3e6bd9d595125e1abbe162e323fd2d06f6f6683185294b79cd2cdb190d5/numpy-1.13.3-cp36-cp36m-manylinux1_x86_64.whl (17.0MB)\n",
      "\u001b[K    100% |████████████████████████████████| 17.0MB 1.7MB/s eta 0:00:01   16% |█████▍                          | 2.9MB 27.4MB/s eta 0:00:01    32% |██████████▎                     | 5.4MB 25.8MB/s eta 0:00:01    39% |████████████▊                   | 6.8MB 31.0MB/s eta 0:00:01    63% |████████████████████▏           | 10.7MB 28.1MB/s eta 0:00:01    78% |█████████████████████████       | 13.3MB 29.3MB/s eta 0:00:01    94% |██████████████████████████████▏ | 16.0MB 27.5MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pandas==0.18.1 (from -r requirements.txt (line 4))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/11/09/e66eb844daba8680ddff26335d5b4fead77f60f957678243549a8dd4830d/pandas-0.18.1.tar.gz (7.3MB)\n",
      "\u001b[K    100% |████████████████████████████████| 7.3MB 7.6MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil==2.6.1 in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 5)) (2.6.1)\n",
      "Requirement already satisfied: pytz==2017.3 in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 6)) (2017.3)\n",
      "Collecting scipy==1.0.0 (from -r requirements.txt (line 7))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d8/5e/caa01ba7be11600b6a9d39265440d7b3be3d69206da887c42bef049521f2/scipy-1.0.0-cp36-cp36m-manylinux1_x86_64.whl (50.0MB)\n",
      "\u001b[K    100% |████████████████████████████████| 50.0MB 659kB/s ta 0:00:011  1% |▋                               | 901kB 23.6MB/s eta 0:00:03    4% |█▎                              | 2.1MB 26.4MB/s eta 0:00:02    13% |████▎                           | 6.7MB 23.3MB/s eta 0:00:02    15% |█████                           | 7.8MB 23.8MB/s eta 0:00:02    20% |██████▌                         | 10.1MB 23.9MB/s eta 0:00:02    26% |████████▋                       | 13.4MB 23.7MB/s eta 0:00:02    28% |█████████▏                      | 14.4MB 20.7MB/s eta 0:00:02    30% |██████████                      | 15.5MB 25.3MB/s eta 0:00:02    33% |██████████▊                     | 16.7MB 23.3MB/s eta 0:00:02    39% |████████████▊                   | 19.9MB 23.4MB/s eta 0:00:02    41% |█████████████▍                  | 20.9MB 24.0MB/s eta 0:00:02    43% |██████████████                  | 21.9MB 20.1MB/s eta 0:00:02    46% |██████████████▊                 | 23.1MB 23.4MB/s eta 0:00:02    48% |███████████████▌                | 24.2MB 21.9MB/s eta 0:00:02    58% |██████████████████▊             | 29.3MB 26.2MB/s eta 0:00:01    63% |████████████████████▎           | 31.7MB 26.2MB/s eta 0:00:01    67% |█████████████████████▋          | 33.8MB 20.8MB/s eta 0:00:01    69% |██████████████████████▏         | 34.7MB 18.6MB/s eta 0:00:01    71% |██████████████████████▉         | 35.7MB 20.1MB/s eta 0:00:01    75% |████████████████████████▎       | 38.0MB 26.6MB/s eta 0:00:01    80% |█████████████████████████▊      | 40.2MB 23.8MB/s eta 0:00:01    86% |███████████████████████████▊    | 43.4MB 24.9MB/s eta 0:00:01    88% |████████████████████████████▌   | 44.5MB 22.1MB/s eta 0:00:01    93% |██████████████████████████████  | 46.9MB 25.6MB/s eta 0:00:01    98% |███████████████████████████████▍| 49.0MB 22.5MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scikit-learn==0.19.1 in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 8)) (0.19.1)\n",
      "Requirement already satisfied: six==1.11.0 in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 9)) (1.11.0)\n",
      "Collecting tables==3.3.0 (from -r requirements.txt (line 10))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/09/e7/72ca83c7bd75db94c23fcaf58debe1be5e9842376c630793e23765cab44b/tables-3.3.0-cp36-cp36m-manylinux1_x86_64.whl (4.6MB)\n",
      "\u001b[K    100% |████████████████████████████████| 4.6MB 9.0MB/s eta 0:00:01    33% |██████████▊                     | 1.5MB 29.5MB/s eta 0:00:01    87% |████████████████████████████    | 4.0MB 29.2MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tqdm==4.19.5 (from -r requirements.txt (line 11))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/3c/341b4fa23cb3abc335207dba057c790f3bb329f6757e1fcd5d347bcf8308/tqdm-4.19.5-py2.py3-none-any.whl (51kB)\n",
      "\u001b[K    100% |████████████████████████████████| 61kB 8.6MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting zipline===1.2.0 (from -r requirements.txt (line 12))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/15/d3/689f2a940478b82ac57c751a40460598221fd82b0449a7a8f7eef47a3bcc/zipline-1.2.0.tar.gz (659kB)\n",
      "\u001b[K    100% |████████████████████████████████| 665kB 13.9MB/s ta 0:00:01    71% |██████████████████████▉         | 471kB 19.9MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: matplotlib>=1.4.0 in /opt/conda/lib/python3.6/site-packages (from alphalens==0.3.2->-r requirements.txt (line 1)) (2.1.0)\n",
      "Requirement already satisfied: seaborn>=0.6.0 in /opt/conda/lib/python3.6/site-packages (from alphalens==0.3.2->-r requirements.txt (line 1)) (0.8.1)\n",
      "Requirement already satisfied: statsmodels>=0.6.1 in /opt/conda/lib/python3.6/site-packages (from alphalens==0.3.2->-r requirements.txt (line 1)) (0.8.0)\n",
      "Requirement already satisfied: IPython>=3.2.3 in /opt/conda/lib/python3.6/site-packages (from alphalens==0.3.2->-r requirements.txt (line 1)) (6.5.0)\n",
      "Requirement already satisfied: numexpr>=2.5.2 in /opt/conda/lib/python3.6/site-packages (from tables==3.3.0->-r requirements.txt (line 10)) (2.6.4)\n",
      "Requirement already satisfied: pip>=7.1.0 in /opt/conda/lib/python3.6/site-packages (from zipline===1.2.0->-r requirements.txt (line 12)) (18.1)\n",
      "Requirement already satisfied: setuptools>18.0 in /opt/conda/lib/python3.6/site-packages (from zipline===1.2.0->-r requirements.txt (line 12)) (38.4.0)\n",
      "Collecting Logbook>=0.12.5 (from zipline===1.2.0->-r requirements.txt (line 12))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6b/3f/f4e6693791efacc1282852fba5392da0649b19416b37422c5489f79a52ea/Logbook-1.5.2.tar.gz (85kB)\n",
      "\u001b[K    100% |████████████████████████████████| 92kB 10.7MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting requests-file>=1.4.1 (from zipline===1.2.0->-r requirements.txt (line 12))\n",
      "  Downloading https://files.pythonhosted.org/packages/23/9c/6e63c23c39e53d3df41c77a3d05a49a42c4e1383a6d2a5e3233161b89dbf/requests_file-1.4.3-py2.py3-none-any.whl\n",
      "Collecting pandas-datareader<0.6,>=0.2.1 (from zipline===1.2.0->-r requirements.txt (line 12))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/c5/cc720f531bbde0efeab940de400d0fcc95e87770a3abcd7f90d6d52a3302/pandas_datareader-0.5.0-py2.py3-none-any.whl (74kB)\n",
      "\u001b[K    100% |████████████████████████████████| 81kB 8.1MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: patsy>=0.4.0 in /opt/conda/lib/python3.6/site-packages (from zipline===1.2.0->-r requirements.txt (line 12)) (0.4.1)\n",
      "Requirement already satisfied: requests>=2.9.1 in /opt/conda/lib/python3.6/site-packages (from zipline===1.2.0->-r requirements.txt (line 12)) (2.18.4)\n",
      "Requirement already satisfied: Cython>=0.25.2 in /opt/conda/lib/python3.6/site-packages (from zipline===1.2.0->-r requirements.txt (line 12)) (0.29.7)\n",
      "Collecting cyordereddict>=0.2.2 (from zipline===1.2.0->-r requirements.txt (line 12))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/1a/364cbfd927be1b743c7f0a985a7f1f7e8a51469619f9fefe4ee9240ba210/cyordereddict-1.0.0.tar.gz (138kB)\n",
      "\u001b[K    100% |████████████████████████████████| 143kB 14.5MB/s ta 0:00:01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25hCollecting bottleneck>=1.0.0 (from zipline===1.2.0->-r requirements.txt (line 12))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/05/ae/cedf5323f398ab4e4ff92d6c431a3e1c6a186f9b41ab3e8258dff786a290/Bottleneck-1.2.1.tar.gz (105kB)\n",
      "\u001b[K    100% |████████████████████████████████| 112kB 11.9MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting contextlib2>=0.4.0 (from zipline===1.2.0->-r requirements.txt (line 12))\n",
      "  Downloading https://files.pythonhosted.org/packages/a2/71/8273a7eeed0aff6a854237ab5453bc9aa67deb49df4832801c21f0ff3782/contextlib2-0.5.5-py2.py3-none-any.whl\n",
      "Requirement already satisfied: decorator>=4.0.0 in /opt/conda/lib/python3.6/site-packages (from zipline===1.2.0->-r requirements.txt (line 12)) (4.0.11)\n",
      "Requirement already satisfied: networkx<2.0,>=1.9.1 in /opt/conda/lib/python3.6/site-packages (from zipline===1.2.0->-r requirements.txt (line 12)) (1.11)\n",
      "Collecting bcolz<1,>=0.12.1 (from zipline===1.2.0->-r requirements.txt (line 12))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6c/8b/1ffa01f872cac36173c5eb95b58c01040d8d25f1b242c48577f4104cd3ab/bcolz-0.12.1.tar.gz (622kB)\n",
      "\u001b[K    100% |████████████████████████████████| 624kB 12.5MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: click>=4.0.0 in /opt/conda/lib/python3.6/site-packages (from zipline===1.2.0->-r requirements.txt (line 12)) (6.7)\n",
      "Requirement already satisfied: toolz>=0.8.2 in /opt/conda/lib/python3.6/site-packages (from zipline===1.2.0->-r requirements.txt (line 12)) (0.8.2)\n",
      "Collecting multipledispatch>=0.4.8 (from zipline===1.2.0->-r requirements.txt (line 12))\n",
      "  Downloading https://files.pythonhosted.org/packages/89/79/429ecef45fd5e4504f7474d4c3c3c4668c267be3370e4c2fd33e61506833/multipledispatch-0.6.0-py3-none-any.whl\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /opt/conda/lib/python3.6/site-packages (from zipline===1.2.0->-r requirements.txt (line 12)) (1.0)\n",
      "Requirement already satisfied: Mako>=1.0.1 in /opt/conda/lib/python3.6/site-packages/Mako-1.0.7-py3.6.egg (from zipline===1.2.0->-r requirements.txt (line 12)) (1.0.7)\n",
      "Requirement already satisfied: sqlalchemy>=1.0.8 in /opt/conda/lib/python3.6/site-packages (from zipline===1.2.0->-r requirements.txt (line 12)) (1.1.13)\n",
      "Collecting alembic>=0.7.7 (from zipline===1.2.0->-r requirements.txt (line 12))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9a/0f/a5e8997d58882da8ecd288360dddf133a83145de6480216774923b393422/alembic-1.1.0.tar.gz (1.0MB)\n",
      "\u001b[K    100% |████████████████████████████████| 1.0MB 6.5MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting sortedcontainers>=1.4.4 (from zipline===1.2.0->-r requirements.txt (line 12))\n",
      "  Downloading https://files.pythonhosted.org/packages/13/f3/cf85f7c3a2dbd1a515d51e1f1676d971abe41bba6f4ab5443240d9a78e5b/sortedcontainers-2.1.0-py2.py3-none-any.whl\n",
      "Collecting intervaltree>=2.1.0 (from zipline===1.2.0->-r requirements.txt (line 12))\n",
      "  Downloading https://files.pythonhosted.org/packages/e8/f9/76237755b2020cd74549e98667210b2dd54d3fb17c6f4a62631e61d31225/intervaltree-3.0.2.tar.gz\n",
      "Collecting lru-dict>=1.1.4 (from zipline===1.2.0->-r requirements.txt (line 12))\n",
      "  Downloading https://files.pythonhosted.org/packages/00/a5/32ed6e10246cd341ca8cc205acea5d208e4053f48a4dced2b1b31d45ba3f/lru-dict-1.1.6.tar.gz\n",
      "Collecting empyrical>=0.4.2 (from zipline===1.2.0->-r requirements.txt (line 12))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/84/9e/9506e8b25464ff57ef93b5ba9092b464b44dc76b717695b126b3c93214a2/empyrical-0.5.3.tar.gz (50kB)\n",
      "\u001b[K    100% |████████████████████████████████| 51kB 9.3MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.6/site-packages/cycler-0.10.0-py3.6.egg (from matplotlib>=1.4.0->alphalens==0.3.2->-r requirements.txt (line 1)) (0.10.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/lib/python3.6/site-packages (from matplotlib>=1.4.0->alphalens==0.3.2->-r requirements.txt (line 1)) (2.2.0)\n",
      "Requirement already satisfied: traitlets>=4.2 in /opt/conda/lib/python3.6/site-packages (from IPython>=3.2.3->alphalens==0.3.2->-r requirements.txt (line 1)) (4.3.2)\n",
      "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /opt/conda/lib/python3.6/site-packages (from IPython>=3.2.3->alphalens==0.3.2->-r requirements.txt (line 1)) (4.3.1)\n",
      "Requirement already satisfied: jedi>=0.10 in /opt/conda/lib/python3.6/site-packages (from IPython>=3.2.3->alphalens==0.3.2->-r requirements.txt (line 1)) (0.10.2)\n",
      "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.15 in /opt/conda/lib/python3.6/site-packages (from IPython>=3.2.3->alphalens==0.3.2->-r requirements.txt (line 1)) (1.0.15)\n",
      "Requirement already satisfied: pygments in /opt/conda/lib/python3.6/site-packages (from IPython>=3.2.3->alphalens==0.3.2->-r requirements.txt (line 1)) (2.2.0)\n",
      "Requirement already satisfied: simplegeneric>0.8 in /opt/conda/lib/python3.6/site-packages (from IPython>=3.2.3->alphalens==0.3.2->-r requirements.txt (line 1)) (0.8.1)\n",
      "Requirement already satisfied: pickleshare in /opt/conda/lib/python3.6/site-packages (from IPython>=3.2.3->alphalens==0.3.2->-r requirements.txt (line 1)) (0.7.4)\n",
      "Requirement already satisfied: backcall in /opt/conda/lib/python3.6/site-packages (from IPython>=3.2.3->alphalens==0.3.2->-r requirements.txt (line 1)) (0.1.0)\n",
      "Collecting requests-ftp (from pandas-datareader<0.6,>=0.2.1->zipline===1.2.0->-r requirements.txt (line 12))\n",
      "  Downloading https://files.pythonhosted.org/packages/3d/ca/14b2ad1e93b5195eeaf56b86b7ecfd5ea2d5754a68d17aeb1e5b9f95b3cf/requests-ftp-0.3.1.tar.gz\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests>=2.9.1->zipline===1.2.0->-r requirements.txt (line 12)) (3.0.4)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests>=2.9.1->zipline===1.2.0->-r requirements.txt (line 12)) (2.6)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests>=2.9.1->zipline===1.2.0->-r requirements.txt (line 12)) (1.22)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests>=2.9.1->zipline===1.2.0->-r requirements.txt (line 12)) (2017.11.5)\n",
      "Collecting python-editor>=0.3 (from alembic>=0.7.7->zipline===1.2.0->-r requirements.txt (line 12))\n",
      "  Downloading https://files.pythonhosted.org/packages/c6/d3/201fc3abe391bbae6606e6f1d598c15d367033332bd54352b12f35513717/python_editor-1.0.4-py3-none-any.whl\n",
      "Requirement already satisfied: ipython-genutils in /opt/conda/lib/python3.6/site-packages (from traitlets>=4.2->IPython>=3.2.3->alphalens==0.3.2->-r requirements.txt (line 1)) (0.2.0)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.6/site-packages (from pexpect; sys_platform != \"win32\"->IPython>=3.2.3->alphalens==0.3.2->-r requirements.txt (line 1)) (0.5.2)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.6/site-packages (from prompt-toolkit<2.0.0,>=1.0.15->IPython>=3.2.3->alphalens==0.3.2->-r requirements.txt (line 1)) (0.1.7)\n",
      "Building wheels for collected packages: alphalens, pandas, zipline, Logbook, cyordereddict, bottleneck, bcolz, alembic, intervaltree, lru-dict, empyrical, requests-ftp\n",
      "  Running setup.py bdist_wheel for alphalens ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/77/1e/9a/223b4c94d7f564f25d94b48ca5b9c53e3034016ece3fd8c8c1\n",
      "  Running setup.py bdist_wheel for pandas ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/a3/08/c3/8fdd52954d4b415624cff43c6dd32a22bac90306976a98f4af\n",
      "  Running setup.py bdist_wheel for zipline ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/5d/20/7d/b48368c8634b1cb6cc7232833b2780a265d4217c0ad2e3d24c\n",
      "  Running setup.py bdist_wheel for Logbook ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/c7/d6/22/9f6bd2884a48b215fdd9cbfa78671b51f4470dfd3d14ef701a\n",
      "  Running setup.py bdist_wheel for cyordereddict ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/0b/9d/8b/5bf3e22c1edd59b50f11bb19dec9dfcfe5a479fc7ace02b61f\n",
      "  Running setup.py bdist_wheel for bottleneck ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/f2/bf/ec/e0f39aa27001525ad455139ee57ec7d0776fe074dfd78c97e4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Running setup.py bdist_wheel for bcolz ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/c5/cc/1b/2cf1f88959af5d7f4d449b7fc6c9452d0ecbd86fd61a9ee376\n",
      "  Running setup.py bdist_wheel for alembic ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/0e/20/17/deeeea8d626ba5ca988fa4d4dd7c12cfbda0fbb3418a5010ff\n",
      "  Running setup.py bdist_wheel for intervaltree ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/08/99/c0/5a5942f5b9567c59c14aac76f95a70bf11dccc71240b91ebf5\n",
      "  Running setup.py bdist_wheel for lru-dict ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/b7/ef/06/fbdd555907a7d438fb33e4c8675f771ff1cf41917284c51ebf\n",
      "  Running setup.py bdist_wheel for empyrical ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/10/a4/3b/951bd609878a82fd72b9ea23699daf1eaada4ff6f583152876\n",
      "  Running setup.py bdist_wheel for requests-ftp ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/2a/98/32/37195e45a3392a73d9f65c488cbea30fe5bad76aaef4d6b020\n",
      "Successfully built alphalens pandas zipline Logbook cyordereddict bottleneck bcolz alembic intervaltree lru-dict empyrical requests-ftp\n",
      "\u001b[31mtensorflow 1.3.0 requires tensorflow-tensorboard<0.2.0,>=0.1.0, which is not installed.\u001b[0m\n",
      "\u001b[31mmoviepy 0.2.3.2 has requirement tqdm==4.11.2, but you'll have tqdm 4.19.5 which is incompatible.\u001b[0m\n",
      "Installing collected packages: numpy, pandas, scipy, alphalens, graphviz, tables, tqdm, Logbook, requests-file, requests-ftp, pandas-datareader, cyordereddict, bottleneck, contextlib2, bcolz, multipledispatch, python-editor, alembic, sortedcontainers, intervaltree, lru-dict, empyrical, zipline\n",
      "  Found existing installation: numpy 1.12.1\n",
      "    Uninstalling numpy-1.12.1:\n",
      "      Successfully uninstalled numpy-1.12.1\n",
      "  Found existing installation: pandas 0.23.3\n",
      "    Uninstalling pandas-0.23.3:\n",
      "      Successfully uninstalled pandas-0.23.3\n",
      "  Found existing installation: scipy 0.19.1\n",
      "    Uninstalling scipy-0.19.1:\n",
      "      Successfully uninstalled scipy-0.19.1\n",
      "  Found existing installation: tqdm 4.11.2\n",
      "    Uninstalling tqdm-4.11.2:\n",
      "      Successfully uninstalled tqdm-4.11.2\n",
      "Successfully installed Logbook-1.5.2 alembic-1.1.0 alphalens-0.3.2 bcolz-0.12.1 bottleneck-1.2.1 contextlib2-0.5.5 cyordereddict-1.0.0 empyrical-0.5.3 graphviz-0.10.1 intervaltree-3.0.2 lru-dict-1.1.6 multipledispatch-0.6.0 numpy-1.13.3 pandas-0.18.1 pandas-datareader-0.5.0 python-editor-1.0.4 requests-file-1.4.3 requests-ftp-0.3.1 scipy-1.0.0 sortedcontainers-2.1.0 tables-3.3.0 tqdm-4.19.5 zipline-1.2.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import project_helper\n",
    "import project_tests\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams['figure.figsize'] = (14, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/workspace/data'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shutil   \n",
    "shutil.copytree('/home/workspace/../../data/', '/home/workspace/data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/workspace/proj_7_data.zip.zip'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shutil.make_archive('proj_7_data.zip', 'zip', 'data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pipeline\n",
    "### Data Bundle\n",
    "We'll be using Zipline to handle our data. We've created a end of day data bundle for this project. Run the cell below to register this data bundle in zipline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Registered\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from zipline.data import bundles\n",
    "\n",
    "os.environ['ZIPLINE_ROOT'] = os.path.join(os.getcwd(), '..', '..', 'data', 'project_7_eod')\n",
    "\n",
    "ingest_func = bundles.csvdir.csvdir_equities(['daily'], project_helper.EOD_BUNDLE_NAME)\n",
    "bundles.register(project_helper.EOD_BUNDLE_NAME, ingest_func)\n",
    "\n",
    "print('Data Registered')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "EmptyDataError",
     "evalue": "No columns to parse from file",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mEmptyDataError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-ecf25ab7bf70>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0meod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'..'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'..'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'project_7_eod'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0meod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'eod.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    560\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    561\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 562\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 315\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mchunksize\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    643\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 645\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m    797\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    798\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 799\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    800\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    801\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1211\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'allow_leading_cols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1213\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_parser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m         \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader.__cinit__ (pandas/parser.c:5214)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mEmptyDataError\u001b[0m: No columns to parse from file"
     ]
    }
   ],
   "source": [
    "eod = pd.read_csv(os.path.join(os.getcwd(), '..', '..', 'data', 'project_7_eod'))\n",
    "eod.to_csv('eod.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Pipeline Engine\n",
    "We'll be using Zipline's pipeline package to access our data for this project. To use it, we must build a pipeline engine. Run the cell below to build the engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipline.pipeline import Pipeline\n",
    "from zipline.pipeline.factors import AverageDollarVolume\n",
    "from zipline.utils.calendars import get_calendar\n",
    "\n",
    "\n",
    "universe = AverageDollarVolume(window_length=120).top(500) \n",
    "trading_calendar = get_calendar('NYSE') \n",
    "bundle_data = bundles.load(project_helper.EOD_BUNDLE_NAME)\n",
    "engine = project_helper.build_pipeline_engine(bundle_data, trading_calendar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View Data\n",
    "With the pipeline engine built, let's get the stocks at the end of the period in the universe we're using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Equity(0 [A]),\n",
       " Equity(1 [AAL]),\n",
       " Equity(2 [AAP]),\n",
       " Equity(3 [AAPL]),\n",
       " Equity(4 [ABBV]),\n",
       " Equity(5 [ABC]),\n",
       " Equity(6 [ABT]),\n",
       " Equity(7 [ACN]),\n",
       " Equity(8 [ADBE]),\n",
       " Equity(9 [ADI]),\n",
       " Equity(10 [ADM]),\n",
       " Equity(11 [ADP]),\n",
       " Equity(12 [ADS]),\n",
       " Equity(13 [ADSK]),\n",
       " Equity(14 [AEE]),\n",
       " Equity(15 [AEP]),\n",
       " Equity(16 [AES]),\n",
       " Equity(17 [AET]),\n",
       " Equity(18 [AFL]),\n",
       " Equity(19 [AGN]),\n",
       " Equity(20 [AIG]),\n",
       " Equity(21 [AIV]),\n",
       " Equity(22 [AIZ]),\n",
       " Equity(23 [AJG]),\n",
       " Equity(24 [AKAM]),\n",
       " Equity(25 [ALB]),\n",
       " Equity(26 [ALGN]),\n",
       " Equity(27 [ALK]),\n",
       " Equity(28 [ALL]),\n",
       " Equity(29 [ALLE]),\n",
       " Equity(30 [ALXN]),\n",
       " Equity(31 [AMAT]),\n",
       " Equity(32 [AMD]),\n",
       " Equity(33 [AME]),\n",
       " Equity(34 [AMG]),\n",
       " Equity(35 [AMGN]),\n",
       " Equity(36 [AMP]),\n",
       " Equity(37 [AMT]),\n",
       " Equity(38 [AMZN]),\n",
       " Equity(39 [ANDV]),\n",
       " Equity(40 [ANSS]),\n",
       " Equity(41 [ANTM]),\n",
       " Equity(42 [AON]),\n",
       " Equity(43 [AOS]),\n",
       " Equity(44 [APA]),\n",
       " Equity(45 [APC]),\n",
       " Equity(46 [APD]),\n",
       " Equity(47 [APH]),\n",
       " Equity(48 [ARE]),\n",
       " Equity(49 [ARNC]),\n",
       " Equity(50 [ATVI]),\n",
       " Equity(51 [AVB]),\n",
       " Equity(52 [AVGO]),\n",
       " Equity(53 [AVY]),\n",
       " Equity(54 [AWK]),\n",
       " Equity(55 [AXP]),\n",
       " Equity(56 [AYI]),\n",
       " Equity(57 [AZO]),\n",
       " Equity(58 [BA]),\n",
       " Equity(59 [BAC]),\n",
       " Equity(60 [BAX]),\n",
       " Equity(61 [BBT]),\n",
       " Equity(62 [BBY]),\n",
       " Equity(63 [BCR]),\n",
       " Equity(64 [BDX]),\n",
       " Equity(65 [BEN]),\n",
       " Equity(66 [BIIB]),\n",
       " Equity(67 [BK]),\n",
       " Equity(68 [BLK]),\n",
       " Equity(69 [BLL]),\n",
       " Equity(70 [BMY]),\n",
       " Equity(71 [BSX]),\n",
       " Equity(72 [BWA]),\n",
       " Equity(73 [BXP]),\n",
       " Equity(74 [C]),\n",
       " Equity(75 [CA]),\n",
       " Equity(76 [CAG]),\n",
       " Equity(77 [CAH]),\n",
       " Equity(78 [CAT]),\n",
       " Equity(79 [CB]),\n",
       " Equity(80 [CBG]),\n",
       " Equity(81 [CBOE]),\n",
       " Equity(82 [CBS]),\n",
       " Equity(83 [CCI]),\n",
       " Equity(84 [CCL]),\n",
       " Equity(85 [CELG]),\n",
       " Equity(86 [CERN]),\n",
       " Equity(87 [CF]),\n",
       " Equity(88 [CFG]),\n",
       " Equity(89 [CHD]),\n",
       " Equity(90 [CHK]),\n",
       " Equity(91 [CHRW]),\n",
       " Equity(92 [CHTR]),\n",
       " Equity(93 [CI]),\n",
       " Equity(94 [CINF]),\n",
       " Equity(95 [CL]),\n",
       " Equity(96 [CLX]),\n",
       " Equity(97 [CMA]),\n",
       " Equity(98 [CMCSA]),\n",
       " Equity(99 [CME]),\n",
       " Equity(100 [CMG]),\n",
       " Equity(101 [CMI]),\n",
       " Equity(102 [CMS]),\n",
       " Equity(103 [CNC]),\n",
       " Equity(104 [CNP]),\n",
       " Equity(105 [COF]),\n",
       " Equity(106 [COG]),\n",
       " Equity(107 [COL]),\n",
       " Equity(108 [COO]),\n",
       " Equity(109 [COP]),\n",
       " Equity(110 [COST]),\n",
       " Equity(111 [COTY]),\n",
       " Equity(112 [CPB]),\n",
       " Equity(113 [CRM]),\n",
       " Equity(114 [CSCO]),\n",
       " Equity(115 [CSRA]),\n",
       " Equity(116 [CSX]),\n",
       " Equity(117 [CTAS]),\n",
       " Equity(118 [CTL]),\n",
       " Equity(119 [CTSH]),\n",
       " Equity(120 [CTXS]),\n",
       " Equity(121 [CVS]),\n",
       " Equity(122 [CVX]),\n",
       " Equity(123 [CXO]),\n",
       " Equity(124 [D]),\n",
       " Equity(125 [DAL]),\n",
       " Equity(126 [DE]),\n",
       " Equity(127 [DFS]),\n",
       " Equity(128 [DG]),\n",
       " Equity(129 [DGX]),\n",
       " Equity(130 [DHI]),\n",
       " Equity(131 [DHR]),\n",
       " Equity(132 [DIS]),\n",
       " Equity(133 [DISCA]),\n",
       " Equity(134 [DISCK]),\n",
       " Equity(135 [DISH]),\n",
       " Equity(136 [DLR]),\n",
       " Equity(137 [DLTR]),\n",
       " Equity(138 [DOV]),\n",
       " Equity(139 [DPS]),\n",
       " Equity(140 [DRE]),\n",
       " Equity(141 [DRI]),\n",
       " Equity(142 [DTE]),\n",
       " Equity(143 [DUK]),\n",
       " Equity(144 [DVA]),\n",
       " Equity(145 [DVN]),\n",
       " Equity(146 [EA]),\n",
       " Equity(147 [EBAY]),\n",
       " Equity(148 [ECL]),\n",
       " Equity(149 [ED]),\n",
       " Equity(150 [EFX]),\n",
       " Equity(151 [EIX]),\n",
       " Equity(152 [EL]),\n",
       " Equity(153 [EMN]),\n",
       " Equity(154 [EMR]),\n",
       " Equity(155 [EOG]),\n",
       " Equity(156 [EQIX]),\n",
       " Equity(157 [EQR]),\n",
       " Equity(158 [EQT]),\n",
       " Equity(159 [ES]),\n",
       " Equity(160 [ESRX]),\n",
       " Equity(161 [ESS]),\n",
       " Equity(162 [ETFC]),\n",
       " Equity(163 [ETN]),\n",
       " Equity(164 [ETR]),\n",
       " Equity(165 [EVHC]),\n",
       " Equity(166 [EW]),\n",
       " Equity(167 [EXC]),\n",
       " Equity(168 [EXPD]),\n",
       " Equity(169 [EXPE]),\n",
       " Equity(170 [EXR]),\n",
       " Equity(171 [F]),\n",
       " Equity(172 [FAST]),\n",
       " Equity(173 [FB]),\n",
       " Equity(174 [FBHS]),\n",
       " Equity(175 [FCX]),\n",
       " Equity(176 [FDX]),\n",
       " Equity(177 [FE]),\n",
       " Equity(178 [FFIV]),\n",
       " Equity(179 [FIS]),\n",
       " Equity(180 [FISV]),\n",
       " Equity(181 [FITB]),\n",
       " Equity(182 [FL]),\n",
       " Equity(183 [FLIR]),\n",
       " Equity(184 [FLR]),\n",
       " Equity(185 [FLS]),\n",
       " Equity(186 [FMC]),\n",
       " Equity(187 [FOX]),\n",
       " Equity(188 [FOXA]),\n",
       " Equity(189 [FRT]),\n",
       " Equity(190 [FTI]),\n",
       " Equity(191 [GD]),\n",
       " Equity(192 [GE]),\n",
       " Equity(193 [GGP]),\n",
       " Equity(194 [GILD]),\n",
       " Equity(195 [GIS]),\n",
       " Equity(196 [GLW]),\n",
       " Equity(197 [GM]),\n",
       " Equity(198 [GOOG]),\n",
       " Equity(199 [GOOGL]),\n",
       " Equity(200 [GPC]),\n",
       " Equity(201 [GPN]),\n",
       " Equity(202 [GPS]),\n",
       " Equity(203 [GRMN]),\n",
       " Equity(204 [GS]),\n",
       " Equity(205 [GT]),\n",
       " Equity(206 [GWW]),\n",
       " Equity(207 [HAL]),\n",
       " Equity(208 [HAS]),\n",
       " Equity(209 [HBAN]),\n",
       " Equity(210 [HBI]),\n",
       " Equity(211 [HCA]),\n",
       " Equity(212 [HCN]),\n",
       " Equity(213 [HCP]),\n",
       " Equity(214 [HD]),\n",
       " Equity(215 [HES]),\n",
       " Equity(216 [HIG]),\n",
       " Equity(217 [HLT]),\n",
       " Equity(218 [HOG]),\n",
       " Equity(219 [HOLX]),\n",
       " Equity(220 [HON]),\n",
       " Equity(221 [HP]),\n",
       " Equity(222 [HPE]),\n",
       " Equity(223 [HPQ]),\n",
       " Equity(224 [HRB]),\n",
       " Equity(225 [HRL]),\n",
       " Equity(226 [HRS]),\n",
       " Equity(227 [HSIC]),\n",
       " Equity(228 [HST]),\n",
       " Equity(229 [HSY]),\n",
       " Equity(230 [HUM]),\n",
       " Equity(231 [IBM]),\n",
       " Equity(232 [ICE]),\n",
       " Equity(233 [IDXX]),\n",
       " Equity(234 [IFF]),\n",
       " Equity(235 [ILMN]),\n",
       " Equity(236 [INCY]),\n",
       " Equity(237 [INFO]),\n",
       " Equity(238 [INTC]),\n",
       " Equity(239 [INTU]),\n",
       " Equity(240 [IP]),\n",
       " Equity(241 [IPG]),\n",
       " Equity(242 [IR]),\n",
       " Equity(243 [IRM]),\n",
       " Equity(244 [ISRG]),\n",
       " Equity(245 [IT]),\n",
       " Equity(246 [ITW]),\n",
       " Equity(247 [IVZ]),\n",
       " Equity(248 [JBHT]),\n",
       " Equity(249 [JCI]),\n",
       " Equity(250 [JEC]),\n",
       " Equity(251 [JNJ]),\n",
       " Equity(252 [JNPR]),\n",
       " Equity(253 [JPM]),\n",
       " Equity(254 [JWN]),\n",
       " Equity(255 [K]),\n",
       " Equity(256 [KEY]),\n",
       " Equity(257 [KHC]),\n",
       " Equity(258 [KIM]),\n",
       " Equity(259 [KLAC]),\n",
       " Equity(260 [KMB]),\n",
       " Equity(261 [KMI]),\n",
       " Equity(262 [KMX]),\n",
       " Equity(263 [KO]),\n",
       " Equity(264 [KORS]),\n",
       " Equity(265 [KR]),\n",
       " Equity(266 [KSS]),\n",
       " Equity(267 [KSU]),\n",
       " Equity(268 [L]),\n",
       " Equity(269 [LB]),\n",
       " Equity(270 [LEG]),\n",
       " Equity(271 [LEN]),\n",
       " Equity(272 [LH]),\n",
       " Equity(273 [LKQ]),\n",
       " Equity(274 [LLL]),\n",
       " Equity(275 [LLY]),\n",
       " Equity(276 [LMT]),\n",
       " Equity(277 [LNC]),\n",
       " Equity(278 [LNT]),\n",
       " Equity(279 [LOW]),\n",
       " Equity(280 [LRCX]),\n",
       " Equity(281 [LUK]),\n",
       " Equity(282 [LUV]),\n",
       " Equity(283 [LVLT]),\n",
       " Equity(284 [LYB]),\n",
       " Equity(285 [M]),\n",
       " Equity(286 [MA]),\n",
       " Equity(287 [MAA]),\n",
       " Equity(288 [MAC]),\n",
       " Equity(289 [MAR]),\n",
       " Equity(290 [MAS]),\n",
       " Equity(291 [MAT]),\n",
       " Equity(292 [MCD]),\n",
       " Equity(293 [MCHP]),\n",
       " Equity(294 [MCK]),\n",
       " Equity(295 [MCO]),\n",
       " Equity(296 [MDLZ]),\n",
       " Equity(297 [MDT]),\n",
       " Equity(298 [MET]),\n",
       " Equity(299 [MGM]),\n",
       " Equity(300 [MHK]),\n",
       " Equity(301 [MKC]),\n",
       " Equity(302 [MLM]),\n",
       " Equity(303 [MMC]),\n",
       " Equity(304 [MNST]),\n",
       " Equity(305 [MO]),\n",
       " Equity(306 [MON]),\n",
       " Equity(307 [MOS]),\n",
       " Equity(308 [MPC]),\n",
       " Equity(309 [MRK]),\n",
       " Equity(310 [MRO]),\n",
       " Equity(311 [MS]),\n",
       " Equity(312 [MSFT]),\n",
       " Equity(313 [MSI]),\n",
       " Equity(314 [MTB]),\n",
       " Equity(315 [MTD]),\n",
       " Equity(316 [MU]),\n",
       " Equity(317 [MYL]),\n",
       " Equity(318 [NAVI]),\n",
       " Equity(319 [NBL]),\n",
       " Equity(320 [NDAQ]),\n",
       " Equity(321 [NEE]),\n",
       " Equity(322 [NEM]),\n",
       " Equity(323 [NFLX]),\n",
       " Equity(324 [NFX]),\n",
       " Equity(325 [NI]),\n",
       " Equity(326 [NKE]),\n",
       " Equity(327 [NLSN]),\n",
       " Equity(328 [NOC]),\n",
       " Equity(329 [NOV]),\n",
       " Equity(330 [NRG]),\n",
       " Equity(331 [NSC]),\n",
       " Equity(332 [NTAP]),\n",
       " Equity(333 [NTRS]),\n",
       " Equity(334 [NUE]),\n",
       " Equity(335 [NVDA]),\n",
       " Equity(336 [NWL]),\n",
       " Equity(337 [NWS]),\n",
       " Equity(338 [NWSA]),\n",
       " Equity(339 [O]),\n",
       " Equity(340 [OKE]),\n",
       " Equity(341 [OMC]),\n",
       " Equity(342 [ORCL]),\n",
       " Equity(343 [ORLY]),\n",
       " Equity(344 [OXY]),\n",
       " Equity(345 [PAYX]),\n",
       " Equity(346 [PBCT]),\n",
       " Equity(347 [PCAR]),\n",
       " Equity(348 [PCG]),\n",
       " Equity(349 [PDCO]),\n",
       " Equity(350 [PEG]),\n",
       " Equity(351 [PEP]),\n",
       " Equity(352 [PFE]),\n",
       " Equity(353 [PFG]),\n",
       " Equity(354 [PG]),\n",
       " Equity(355 [PGR]),\n",
       " Equity(356 [PH]),\n",
       " Equity(357 [PHM]),\n",
       " Equity(358 [PKG]),\n",
       " Equity(359 [PKI]),\n",
       " Equity(360 [PLD]),\n",
       " Equity(361 [PM]),\n",
       " Equity(362 [PNC]),\n",
       " Equity(363 [PNR]),\n",
       " Equity(364 [PNW]),\n",
       " Equity(365 [PPG]),\n",
       " Equity(366 [PPL]),\n",
       " Equity(367 [PRGO]),\n",
       " Equity(368 [PRU]),\n",
       " Equity(369 [PSA]),\n",
       " Equity(370 [PSX]),\n",
       " Equity(371 [PVH]),\n",
       " Equity(372 [PWR]),\n",
       " Equity(373 [PX]),\n",
       " Equity(374 [PXD]),\n",
       " Equity(375 [PYPL]),\n",
       " Equity(376 [QCOM]),\n",
       " Equity(377 [QRVO]),\n",
       " Equity(378 [RCL]),\n",
       " Equity(379 [RE]),\n",
       " Equity(380 [REG]),\n",
       " Equity(381 [REGN]),\n",
       " Equity(382 [RF]),\n",
       " Equity(383 [RHI]),\n",
       " Equity(384 [RHT]),\n",
       " Equity(385 [RJF]),\n",
       " Equity(386 [RL]),\n",
       " Equity(387 [RMD]),\n",
       " Equity(388 [ROK]),\n",
       " Equity(389 [ROP]),\n",
       " Equity(390 [ROST]),\n",
       " Equity(391 [RRC]),\n",
       " Equity(392 [RSG]),\n",
       " Equity(393 [RTN]),\n",
       " Equity(394 [SBAC]),\n",
       " Equity(395 [SBUX]),\n",
       " Equity(396 [SCG]),\n",
       " Equity(397 [SCHW]),\n",
       " Equity(398 [SEE]),\n",
       " Equity(399 [SHW]),\n",
       " Equity(400 [SIG]),\n",
       " Equity(401 [SJM]),\n",
       " Equity(402 [SLB]),\n",
       " Equity(403 [SLG]),\n",
       " Equity(404 [SNA]),\n",
       " Equity(405 [SNI]),\n",
       " Equity(406 [SNPS]),\n",
       " Equity(407 [SO]),\n",
       " Equity(408 [SPG]),\n",
       " Equity(409 [SPLS]),\n",
       " Equity(410 [SRCL]),\n",
       " Equity(411 [SRE]),\n",
       " Equity(412 [STI]),\n",
       " Equity(413 [STT]),\n",
       " Equity(414 [STX]),\n",
       " Equity(415 [STZ]),\n",
       " Equity(416 [SWK]),\n",
       " Equity(417 [SWKS]),\n",
       " Equity(418 [SYF]),\n",
       " Equity(419 [SYK]),\n",
       " Equity(420 [SYMC]),\n",
       " Equity(421 [SYY]),\n",
       " Equity(422 [T]),\n",
       " Equity(423 [TAP]),\n",
       " Equity(424 [TDG]),\n",
       " Equity(425 [TEL]),\n",
       " Equity(426 [TGT]),\n",
       " Equity(427 [TIF]),\n",
       " Equity(428 [TJX]),\n",
       " Equity(429 [TMK]),\n",
       " Equity(430 [TMO]),\n",
       " Equity(431 [TRIP]),\n",
       " Equity(432 [TROW]),\n",
       " Equity(433 [TRV]),\n",
       " Equity(434 [TSCO]),\n",
       " Equity(435 [TSN]),\n",
       " Equity(436 [TSS]),\n",
       " Equity(437 [TWX]),\n",
       " Equity(438 [TXN]),\n",
       " Equity(439 [TXT]),\n",
       " Equity(440 [UAA]),\n",
       " Equity(441 [UAL]),\n",
       " Equity(442 [UDR]),\n",
       " Equity(443 [UHS]),\n",
       " Equity(444 [ULTA]),\n",
       " Equity(445 [UNH]),\n",
       " Equity(446 [UNM]),\n",
       " Equity(447 [UNP]),\n",
       " Equity(448 [UPS]),\n",
       " Equity(449 [URI]),\n",
       " Equity(450 [USB]),\n",
       " Equity(451 [UTX]),\n",
       " Equity(452 [V]),\n",
       " Equity(453 [VAR]),\n",
       " Equity(454 [VFC]),\n",
       " Equity(455 [VIAB]),\n",
       " Equity(456 [VLO]),\n",
       " Equity(457 [VMC]),\n",
       " Equity(458 [VNO]),\n",
       " Equity(459 [VRSK]),\n",
       " Equity(460 [VRSN]),\n",
       " Equity(461 [VRTX]),\n",
       " Equity(462 [VTR]),\n",
       " Equity(463 [VZ]),\n",
       " Equity(464 [WAT]),\n",
       " Equity(465 [WBA]),\n",
       " Equity(466 [WDC]),\n",
       " Equity(467 [WEC]),\n",
       " Equity(468 [WFC]),\n",
       " Equity(469 [WHR]),\n",
       " Equity(471 [WM]),\n",
       " Equity(472 [WMB]),\n",
       " Equity(473 [WMT]),\n",
       " Equity(474 [WRK]),\n",
       " Equity(475 [WU]),\n",
       " Equity(476 [WY]),\n",
       " Equity(477 [WYN]),\n",
       " Equity(478 [WYNN]),\n",
       " Equity(479 [XEC]),\n",
       " Equity(480 [XEL]),\n",
       " Equity(481 [XL]),\n",
       " Equity(482 [XLNX]),\n",
       " Equity(483 [XOM]),\n",
       " Equity(484 [XRAY]),\n",
       " Equity(485 [XRX]),\n",
       " Equity(486 [XYL]),\n",
       " Equity(487 [YUM]),\n",
       " Equity(488 [ZBH]),\n",
       " Equity(489 [ZION]),\n",
       " Equity(490 [ZTS])]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "universe_end_date = pd.Timestamp('2016-01-05', tz='UTC')\n",
    "\n",
    "universe_tickers = engine\\\n",
    "    .run_pipeline(\n",
    "        Pipeline(screen=universe),\n",
    "        universe_end_date,\n",
    "        universe_end_date)\\\n",
    "    .index.get_level_values(1)\\\n",
    "    .values.tolist()\n",
    "    \n",
    "universe_tickers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Returns\n",
    "Not that we have our pipeline built, let's access the returns data. We'll start by building a data portal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipline.data.data_portal import DataPortal\n",
    "\n",
    "\n",
    "data_portal = DataPortal(\n",
    "    bundle_data.asset_finder,\n",
    "    trading_calendar=trading_calendar,\n",
    "    first_trading_day=bundle_data.equity_daily_bar_reader.first_trading_day,\n",
    "    equity_minute_reader=None,\n",
    "    equity_daily_reader=bundle_data.equity_daily_bar_reader,\n",
    "    adjustment_reader=bundle_data.adjustment_reader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make the code easier to read, we've built the helper function `get_pricing` to get the pricing from the data portal. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pricing(data_portal, trading_calendar, assets, start_date, end_date, field='close'):\n",
    "    end_dt = pd.Timestamp(end_date.strftime('%Y-%m-%d'), tz='UTC', offset='C')\n",
    "    start_dt = pd.Timestamp(start_date.strftime('%Y-%m-%d'), tz='UTC', offset='C')\n",
    "\n",
    "    end_loc = trading_calendar.closes.index.get_loc(end_dt)\n",
    "    start_loc = trading_calendar.closes.index.get_loc(start_dt)\n",
    "\n",
    "    return data_portal.get_history_window(\n",
    "        assets=assets,\n",
    "        end_dt=end_dt,\n",
    "        bar_count=end_loc - start_loc,\n",
    "        frequency='1d',\n",
    "        field=field,\n",
    "        data_frequency='daily')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alpha Factors\n",
    "It's time to start working on the alpha factors. In this project, we'll use the following factors:\n",
    "- Momentum 1 Year Factor\n",
    "- Mean Reversion 5 Day Sector Neutral Smoothed Factor\n",
    "- Overnight Sentiment Smoothed Factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipline.pipeline.factors import CustomFactor, DailyReturns, Returns, SimpleMovingAverage, AnnualizedVolatility\n",
    "from zipline.pipeline.data import USEquityPricing\n",
    "\n",
    "\n",
    "factor_start_date = universe_end_date - pd.DateOffset(years=3, days=2)\n",
    "sector = project_helper.Sector()\n",
    "\n",
    "def momentum_1yr(window_length, universe, sector):\n",
    "    return Returns(window_length=window_length, mask=universe) \\\n",
    "        .demean(groupby=sector) \\\n",
    "        .rank() \\\n",
    "        .zscore()\n",
    "\n",
    "def mean_reversion_5day_sector_neutral_smoothed(window_length, universe, sector):\n",
    "    unsmoothed_factor = -Returns(window_length=window_length, mask=universe) \\\n",
    "        .demean(groupby=sector) \\\n",
    "        .rank() \\\n",
    "        .zscore()\n",
    "    return SimpleMovingAverage(inputs=[unsmoothed_factor], window_length=window_length) \\\n",
    "        .rank() \\\n",
    "        .zscore()\n",
    "\n",
    "class CTO(Returns):\n",
    "    \"\"\"\n",
    "    Computes the overnight return, per hypothesis from\n",
    "    https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2554010\n",
    "    \"\"\"\n",
    "    inputs = [USEquityPricing.open, USEquityPricing.close]\n",
    "    \n",
    "    def compute(self, today, assets, out, opens, closes):\n",
    "        \"\"\"\n",
    "        The opens and closes matrix is 2 rows x N assets, with the most recent at the bottom.\n",
    "        As such, opens[-1] is the most recent open, and closes[0] is the earlier close\n",
    "        \"\"\"\n",
    "        out[:] = (opens[-1] - closes[0]) / closes[0]\n",
    "        \n",
    "class TrailingOvernightReturns(Returns):\n",
    "    \"\"\"\n",
    "    Sum of trailing 1m O/N returns\n",
    "    \"\"\"\n",
    "    window_safe = True\n",
    "    \n",
    "    def compute(self, today, asset_ids, out, cto):\n",
    "        out[:] = np.nansum(cto, axis=0)\n",
    "\n",
    "def overnight_sentiment_smoothed(cto_window_length, trail_overnight_returns_window_length, universe):\n",
    "    cto_out = CTO(mask=universe, window_length=cto_window_length)\n",
    "    unsmoothed_factor = TrailingOvernightReturns(inputs=[cto_out], window_length=trail_overnight_returns_window_length) \\\n",
    "        .rank() \\\n",
    "        .zscore()\n",
    "    return SimpleMovingAverage(inputs=[unsmoothed_factor], window_length=trail_overnight_returns_window_length) \\\n",
    "        .rank() \\\n",
    "        .zscore()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine the Factors to a single Pipeline\n",
    "Let's add the factors to a pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "universe = AverageDollarVolume(window_length=120).top(500)\n",
    "sector = project_helper.Sector()\n",
    "\n",
    "pipeline = Pipeline(screen=universe)\n",
    "pipeline.add(\n",
    "    momentum_1yr(252, universe, sector),\n",
    "    'Momentum_1YR')\n",
    "pipeline.add(\n",
    "    mean_reversion_5day_sector_neutral_smoothed(20, universe, sector),\n",
    "    'Mean_Reversion_Sector_Neutral_Smoothed')\n",
    "pipeline.add(\n",
    "    overnight_sentiment_smoothed(2, 10, universe),\n",
    "    'Overnight_Sentiment_Smoothed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features and Labels\n",
    "Let's create some features that we think will help the model make predictions.\n",
    "### \"Universal\" Quant Features\n",
    "To capture the universe, we'll use the following as features:\n",
    "- Stock Volatility 20d, 120d\n",
    "- Stock Dollar Volume 20d, 120d\n",
    "- Sector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.add(AnnualizedVolatility(window_length=20, mask=universe).rank().zscore(), 'volatility_20d')\n",
    "pipeline.add(AnnualizedVolatility(window_length=120, mask=universe).rank().zscore(), 'volatility_120d')\n",
    "pipeline.add(AverageDollarVolume(window_length=20, mask=universe).rank().zscore(), 'adv_20d')\n",
    "pipeline.add(AverageDollarVolume(window_length=120, mask=universe).rank().zscore(), 'adv_120d')\n",
    "pipeline.add(sector, 'sector_code')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regime Features\n",
    "We are going to try to capture market-wide regimes. To do that, we'll use the following features:\n",
    "- High and low volatility 20d, 120d\n",
    "- High and low dispersion 20d, 120d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarketDispersion(CustomFactor):\n",
    "    inputs = [DailyReturns()]\n",
    "    window_length = 1\n",
    "    window_safe = True\n",
    "\n",
    "    def compute(self, today, assets, out, returns):\n",
    "        # returns are days in rows, assets across columns\n",
    "        out[:] = np.sqrt(np.nanmean((returns - np.nanmean(returns))**2))\n",
    "\n",
    "\n",
    "pipeline.add(SimpleMovingAverage(inputs=[MarketDispersion(mask=universe)], window_length=20), 'dispersion_20d')\n",
    "pipeline.add(SimpleMovingAverage(inputs=[MarketDispersion(mask=universe)], window_length=120), 'dispersion_120d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarketVolatility(CustomFactor):\n",
    "    inputs = [DailyReturns()]\n",
    "    window_length = 1\n",
    "    window_safe = True\n",
    "    \n",
    "    def compute(self, today, assets, out, returns):\n",
    "        mkt_returns = np.nanmean(returns, axis=1)\n",
    "        out[:] = np.sqrt(260.* np.nanmean((mkt_returns-np.nanmean(mkt_returns))**2))\n",
    "\n",
    "\n",
    "pipeline.add(MarketVolatility(window_length=20), 'market_vol_20d')\n",
    "pipeline.add(MarketVolatility(window_length=120), 'market_vol_120d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target\n",
    "Let's try to predict the go forward 1-week return. When doing this, it's important to quantize the target. The factor we create is the trailing 5-day return."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.add(Returns(window_length=5, mask=universe).quantiles(2), 'return_5d')\n",
    "pipeline.add(Returns(window_length=5, mask=universe).quantiles(25), 'return_5d_p')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Date Features\n",
    "Let's make columns for the trees to split on that might capture trader/investor behavior due to calendar anomalies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Mean_Reversion_Sector_Neutral_Smoothed</th>\n",
       "      <th>Momentum_1YR</th>\n",
       "      <th>Overnight_Sentiment_Smoothed</th>\n",
       "      <th>adv_120d</th>\n",
       "      <th>adv_20d</th>\n",
       "      <th>dispersion_120d</th>\n",
       "      <th>dispersion_20d</th>\n",
       "      <th>market_vol_120d</th>\n",
       "      <th>market_vol_20d</th>\n",
       "      <th>return_5d</th>\n",
       "      <th>...</th>\n",
       "      <th>volatility_20d</th>\n",
       "      <th>is_Janaury</th>\n",
       "      <th>is_December</th>\n",
       "      <th>weekday</th>\n",
       "      <th>quarter</th>\n",
       "      <th>qtr_yr</th>\n",
       "      <th>month_end</th>\n",
       "      <th>month_start</th>\n",
       "      <th>qtr_end</th>\n",
       "      <th>qtr_start</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">2013-01-03 00:00:00+00:00</th>\n",
       "      <th>Equity(0 [A])</th>\n",
       "      <td>-0.26276899</td>\n",
       "      <td>-1.20797813</td>\n",
       "      <td>-1.48566901</td>\n",
       "      <td>1.33857307</td>\n",
       "      <td>1.39741144</td>\n",
       "      <td>0.01326964</td>\n",
       "      <td>0.01117804</td>\n",
       "      <td>0.12966421</td>\n",
       "      <td>0.13758558</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.21980876</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1_2013</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Equity(1 [AAL])</th>\n",
       "      <td>0.09992624</td>\n",
       "      <td>1.71347052</td>\n",
       "      <td>0.91934963</td>\n",
       "      <td>1.13999355</td>\n",
       "      <td>1.08115517</td>\n",
       "      <td>0.01326964</td>\n",
       "      <td>0.01117804</td>\n",
       "      <td>0.12966421</td>\n",
       "      <td>0.13758558</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1.56621970</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1_2013</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Equity(2 [AAP])</th>\n",
       "      <td>1.66913824</td>\n",
       "      <td>-1.53506144</td>\n",
       "      <td>1.50773340</td>\n",
       "      <td>-0.30154668</td>\n",
       "      <td>-0.91934963</td>\n",
       "      <td>0.01326964</td>\n",
       "      <td>0.01117804</td>\n",
       "      <td>0.12966421</td>\n",
       "      <td>0.13758558</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.47040391</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1_2013</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Equity(3 [AAPL])</th>\n",
       "      <td>1.69874602</td>\n",
       "      <td>1.19311071</td>\n",
       "      <td>-1.36799226</td>\n",
       "      <td>1.72837731</td>\n",
       "      <td>1.72837731</td>\n",
       "      <td>0.01326964</td>\n",
       "      <td>0.01117804</td>\n",
       "      <td>0.12966421</td>\n",
       "      <td>0.13758558</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1.61781282</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1_2013</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Equity(4 [ABBV])</th>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>-0.25006310</td>\n",
       "      <td>-1.72837731</td>\n",
       "      <td>-1.64747455</td>\n",
       "      <td>0.01459524</td>\n",
       "      <td>0.01459524</td>\n",
       "      <td>0.12966421</td>\n",
       "      <td>0.13758558</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>nan</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1_2013</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Mean_Reversion_Sector_Neutral_Smoothed  \\\n",
       "2013-01-03 00:00:00+00:00 Equity(0 [A])                                -0.26276899   \n",
       "                          Equity(1 [AAL])                               0.09992624   \n",
       "                          Equity(2 [AAP])                               1.66913824   \n",
       "                          Equity(3 [AAPL])                              1.69874602   \n",
       "                          Equity(4 [ABBV])                                     nan   \n",
       "\n",
       "                                            Momentum_1YR  \\\n",
       "2013-01-03 00:00:00+00:00 Equity(0 [A])      -1.20797813   \n",
       "                          Equity(1 [AAL])     1.71347052   \n",
       "                          Equity(2 [AAP])    -1.53506144   \n",
       "                          Equity(3 [AAPL])    1.19311071   \n",
       "                          Equity(4 [ABBV])           nan   \n",
       "\n",
       "                                            Overnight_Sentiment_Smoothed  \\\n",
       "2013-01-03 00:00:00+00:00 Equity(0 [A])                      -1.48566901   \n",
       "                          Equity(1 [AAL])                     0.91934963   \n",
       "                          Equity(2 [AAP])                     1.50773340   \n",
       "                          Equity(3 [AAPL])                   -1.36799226   \n",
       "                          Equity(4 [ABBV])                   -0.25006310   \n",
       "\n",
       "                                              adv_120d     adv_20d  \\\n",
       "2013-01-03 00:00:00+00:00 Equity(0 [A])     1.33857307  1.39741144   \n",
       "                          Equity(1 [AAL])   1.13999355  1.08115517   \n",
       "                          Equity(2 [AAP])  -0.30154668 -0.91934963   \n",
       "                          Equity(3 [AAPL])  1.72837731  1.72837731   \n",
       "                          Equity(4 [ABBV]) -1.72837731 -1.64747455   \n",
       "\n",
       "                                            dispersion_120d  dispersion_20d  \\\n",
       "2013-01-03 00:00:00+00:00 Equity(0 [A])          0.01326964      0.01117804   \n",
       "                          Equity(1 [AAL])        0.01326964      0.01117804   \n",
       "                          Equity(2 [AAP])        0.01326964      0.01117804   \n",
       "                          Equity(3 [AAPL])       0.01326964      0.01117804   \n",
       "                          Equity(4 [ABBV])       0.01459524      0.01459524   \n",
       "\n",
       "                                            market_vol_120d  market_vol_20d  \\\n",
       "2013-01-03 00:00:00+00:00 Equity(0 [A])          0.12966421      0.13758558   \n",
       "                          Equity(1 [AAL])        0.12966421      0.13758558   \n",
       "                          Equity(2 [AAP])        0.12966421      0.13758558   \n",
       "                          Equity(3 [AAPL])       0.12966421      0.13758558   \n",
       "                          Equity(4 [ABBV])       0.12966421      0.13758558   \n",
       "\n",
       "                                            return_5d    ...     \\\n",
       "2013-01-03 00:00:00+00:00 Equity(0 [A])             0    ...      \n",
       "                          Equity(1 [AAL])           1    ...      \n",
       "                          Equity(2 [AAP])           0    ...      \n",
       "                          Equity(3 [AAPL])          1    ...      \n",
       "                          Equity(4 [ABBV])         -1    ...      \n",
       "\n",
       "                                            volatility_20d  is_Janaury  \\\n",
       "2013-01-03 00:00:00+00:00 Equity(0 [A])        -1.21980876        True   \n",
       "                          Equity(1 [AAL])       1.56621970        True   \n",
       "                          Equity(2 [AAP])      -1.47040391        True   \n",
       "                          Equity(3 [AAPL])      1.61781282        True   \n",
       "                          Equity(4 [ABBV])             nan        True   \n",
       "\n",
       "                                            is_December  weekday quarter  \\\n",
       "2013-01-03 00:00:00+00:00 Equity(0 [A])           False        3       1   \n",
       "                          Equity(1 [AAL])         False        3       1   \n",
       "                          Equity(2 [AAP])         False        3       1   \n",
       "                          Equity(3 [AAPL])        False        3       1   \n",
       "                          Equity(4 [ABBV])        False        3       1   \n",
       "\n",
       "                                            qtr_yr  month_end  month_start  \\\n",
       "2013-01-03 00:00:00+00:00 Equity(0 [A])     1_2013      False        False   \n",
       "                          Equity(1 [AAL])   1_2013      False        False   \n",
       "                          Equity(2 [AAP])   1_2013      False        False   \n",
       "                          Equity(3 [AAPL])  1_2013      False        False   \n",
       "                          Equity(4 [ABBV])  1_2013      False        False   \n",
       "\n",
       "                                           qtr_end qtr_start  \n",
       "2013-01-03 00:00:00+00:00 Equity(0 [A])      False     False  \n",
       "                          Equity(1 [AAL])    False     False  \n",
       "                          Equity(2 [AAP])    False     False  \n",
       "                          Equity(3 [AAPL])   False     False  \n",
       "                          Equity(4 [ABBV])   False     False  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_factors = engine.run_pipeline(pipeline, factor_start_date, universe_end_date)\n",
    "\n",
    "all_factors['is_Janaury'] = all_factors.index.get_level_values(0).month == 1\n",
    "all_factors['is_December'] = all_factors.index.get_level_values(0).month == 12\n",
    "all_factors['weekday'] = all_factors.index.get_level_values(0).weekday\n",
    "all_factors['quarter'] = all_factors.index.get_level_values(0).quarter\n",
    "all_factors['qtr_yr'] = all_factors.quarter.astype('str') + '_' + all_factors.index.get_level_values(0).year.astype('str')\n",
    "all_factors['month_end'] = all_factors.index.get_level_values(0).isin(pd.date_range(start=factor_start_date, end=universe_end_date, freq='BM'))\n",
    "all_factors['month_start'] = all_factors.index.get_level_values(0).isin(pd.date_range(start=factor_start_date, end=universe_end_date, freq='BMS'))\n",
    "all_factors['qtr_end'] = all_factors.index.get_level_values(0).isin(pd.date_range(start=factor_start_date, end=universe_end_date, freq='BQ'))\n",
    "all_factors['qtr_start'] = all_factors.index.get_level_values(0).isin(pd.date_range(start=factor_start_date, end=universe_end_date, freq='BQS'))\n",
    "\n",
    "all_factors.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Hot Encode Sectors\n",
    "For the model to better understand the sector data, we'll one hot encode this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>sector_Healthcare</th>\n",
       "      <th>sector_Technology</th>\n",
       "      <th>sector_Consumer Defensive</th>\n",
       "      <th>sector_Industrials</th>\n",
       "      <th>sector_Utilities</th>\n",
       "      <th>sector_Financial Services</th>\n",
       "      <th>sector_Real Estate</th>\n",
       "      <th>sector_Communication Services</th>\n",
       "      <th>sector_Consumer Cyclical</th>\n",
       "      <th>sector_Energy</th>\n",
       "      <th>sector_Basic Materials</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">2013-01-03 00:00:00+00:00</th>\n",
       "      <th>Equity(0 [A])</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Equity(1 [AAL])</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Equity(2 [AAP])</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Equity(3 [AAPL])</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Equity(4 [ABBV])</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           sector_Healthcare  \\\n",
       "2013-01-03 00:00:00+00:00 Equity(0 [A])                 True   \n",
       "                          Equity(1 [AAL])              False   \n",
       "                          Equity(2 [AAP])              False   \n",
       "                          Equity(3 [AAPL])             False   \n",
       "                          Equity(4 [ABBV])              True   \n",
       "\n",
       "                                           sector_Technology  \\\n",
       "2013-01-03 00:00:00+00:00 Equity(0 [A])                False   \n",
       "                          Equity(1 [AAL])              False   \n",
       "                          Equity(2 [AAP])              False   \n",
       "                          Equity(3 [AAPL])              True   \n",
       "                          Equity(4 [ABBV])             False   \n",
       "\n",
       "                                           sector_Consumer Defensive  \\\n",
       "2013-01-03 00:00:00+00:00 Equity(0 [A])                        False   \n",
       "                          Equity(1 [AAL])                      False   \n",
       "                          Equity(2 [AAP])                      False   \n",
       "                          Equity(3 [AAPL])                     False   \n",
       "                          Equity(4 [ABBV])                     False   \n",
       "\n",
       "                                           sector_Industrials  \\\n",
       "2013-01-03 00:00:00+00:00 Equity(0 [A])                 False   \n",
       "                          Equity(1 [AAL])                True   \n",
       "                          Equity(2 [AAP])               False   \n",
       "                          Equity(3 [AAPL])              False   \n",
       "                          Equity(4 [ABBV])              False   \n",
       "\n",
       "                                           sector_Utilities  \\\n",
       "2013-01-03 00:00:00+00:00 Equity(0 [A])               False   \n",
       "                          Equity(1 [AAL])             False   \n",
       "                          Equity(2 [AAP])             False   \n",
       "                          Equity(3 [AAPL])            False   \n",
       "                          Equity(4 [ABBV])            False   \n",
       "\n",
       "                                           sector_Financial Services  \\\n",
       "2013-01-03 00:00:00+00:00 Equity(0 [A])                        False   \n",
       "                          Equity(1 [AAL])                      False   \n",
       "                          Equity(2 [AAP])                      False   \n",
       "                          Equity(3 [AAPL])                     False   \n",
       "                          Equity(4 [ABBV])                     False   \n",
       "\n",
       "                                           sector_Real Estate  \\\n",
       "2013-01-03 00:00:00+00:00 Equity(0 [A])                 False   \n",
       "                          Equity(1 [AAL])               False   \n",
       "                          Equity(2 [AAP])               False   \n",
       "                          Equity(3 [AAPL])              False   \n",
       "                          Equity(4 [ABBV])              False   \n",
       "\n",
       "                                           sector_Communication Services  \\\n",
       "2013-01-03 00:00:00+00:00 Equity(0 [A])                            False   \n",
       "                          Equity(1 [AAL])                          False   \n",
       "                          Equity(2 [AAP])                          False   \n",
       "                          Equity(3 [AAPL])                         False   \n",
       "                          Equity(4 [ABBV])                         False   \n",
       "\n",
       "                                           sector_Consumer Cyclical  \\\n",
       "2013-01-03 00:00:00+00:00 Equity(0 [A])                       False   \n",
       "                          Equity(1 [AAL])                     False   \n",
       "                          Equity(2 [AAP])                      True   \n",
       "                          Equity(3 [AAPL])                    False   \n",
       "                          Equity(4 [ABBV])                    False   \n",
       "\n",
       "                                           sector_Energy  \\\n",
       "2013-01-03 00:00:00+00:00 Equity(0 [A])            False   \n",
       "                          Equity(1 [AAL])          False   \n",
       "                          Equity(2 [AAP])          False   \n",
       "                          Equity(3 [AAPL])         False   \n",
       "                          Equity(4 [ABBV])         False   \n",
       "\n",
       "                                           sector_Basic Materials  \n",
       "2013-01-03 00:00:00+00:00 Equity(0 [A])                     False  \n",
       "                          Equity(1 [AAL])                   False  \n",
       "                          Equity(2 [AAP])                   False  \n",
       "                          Equity(3 [AAPL])                  False  \n",
       "                          Equity(4 [ABBV])                  False  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sector_lookup = pd.read_csv(\n",
    "    os.path.join(os.getcwd(), '..', '..', 'data', 'project_7_sector', 'labels.csv'),\n",
    "    index_col='Sector_i')['Sector'].to_dict()\n",
    "sector_lookup\n",
    "\n",
    "sector_columns = []\n",
    "for sector_i, sector_name in sector_lookup.items():\n",
    "    secotr_column = 'sector_{}'.format(sector_name)\n",
    "    sector_columns.append(secotr_column)\n",
    "    all_factors[secotr_column] = (all_factors['sector_code'] == sector_i)\n",
    "\n",
    "all_factors[sector_columns].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shift Target\n",
    "We'll use shifted 5 day returns for training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_factors['target'] = all_factors.groupby(level=1)['return_5d'].shift(-5)\n",
    "\n",
    "all_factors[['return_5d','target']].reset_index().sort_values(['level_1', 'level_0']).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IID Check of Target\n",
    "Let's see if the returns are independent and identically distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "\n",
    "\n",
    "def sp(group, col1_name, col2_name):\n",
    "    x = group[col1_name]\n",
    "    y = group[col2_name]\n",
    "    return spearmanr(x, y)[0]\n",
    "\n",
    "\n",
    "all_factors['target_p'] = all_factors.groupby(level=1)['return_5d_p'].shift(-5)\n",
    "all_factors['target_1'] = all_factors.groupby(level=1)['return_5d'].shift(-4)\n",
    "all_factors['target_2'] = all_factors.groupby(level=1)['return_5d'].shift(-3)\n",
    "all_factors['target_3'] = all_factors.groupby(level=1)['return_5d'].shift(-2)\n",
    "all_factors['target_4'] = all_factors.groupby(level=1)['return_5d'].shift(-1)\n",
    "\n",
    "g = all_factors.dropna().groupby(level=0)\n",
    "for i in range(4):\n",
    "    label = 'target_'+str(i+1)\n",
    "    ic = g.apply(sp, 'target', label)\n",
    "    ic.plot(ylim=(-1, 1), label=label)\n",
    "plt.legend(bbox_to_anchor=(1.04, 1), borderaxespad=0)\n",
    "plt.title('Rolling Autocorrelation of Labels Shifted 1,2,3,4 Days')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: What do you observe in the rolling autocorrelation of labels shifted?\n",
    "*#TODO: Put Answer In this Cell*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Valid/Test Splits\n",
    "Now let's split the data into a train, validation, and test dataset. Implement the function `train_valid_test_split` to split the input samples, `all_x`, and targets values, `all_y` into a train, validation, and test dataset. The proportion sizes are `train_size`, `valid_size`, `test_size` respectively.\n",
    "\n",
    "When splitting, make sure the data is in order from train, validation, and test respectivly. Say `train_size` is 0.7, `valid_size` is 0.2, and `test_size` is 0.1. The first 70 percent of `all_x` and `all_y` would be the train set. The next 20 percent of `all_x` and `all_y` would be the validation set. The last 10 percent of `all_x` and `all_y` would be the test set. Make sure not split a day between multiple datasets. It should be contained within a single dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_valid_test_split(all_x, all_y, train_size, valid_size, test_size):\n",
    "    \"\"\"\n",
    "    Generate the train, validation, and test dataset.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    all_x : DataFrame\n",
    "        All the input samples\n",
    "    all_y : Pandas Series\n",
    "        All the target values\n",
    "    train_size : float\n",
    "        The proportion of the data used for the training dataset\n",
    "    valid_size : float\n",
    "        The proportion of the data used for the validation dataset\n",
    "    test_size : float\n",
    "        The proportion of the data used for the test dataset\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    x_train : DataFrame\n",
    "        The train input samples\n",
    "    x_valid : DataFrame\n",
    "        The validation input samples\n",
    "    x_test : DataFrame\n",
    "        The test input samples\n",
    "    y_train : Pandas Series\n",
    "        The train target values\n",
    "    y_valid : Pandas Series\n",
    "        The validation target values\n",
    "    y_test : Pandas Series\n",
    "        The test target values\n",
    "    \"\"\"\n",
    "    assert train_size >= 0 and train_size <= 1.0\n",
    "    assert valid_size >= 0 and valid_size <= 1.0\n",
    "    assert test_size >= 0 and test_size <= 1.0\n",
    "    assert train_size + valid_size + test_size == 1.0\n",
    "    \n",
    "    # TODO: Implement\n",
    "    \n",
    "    return None, None, None, None, None, None\n",
    "\n",
    "\n",
    "project_tests.test_train_valid_test_split(train_valid_test_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With `train_valid_test_split` implemented, let's split the data into a train, validation, and test set. For this, we'll use some of the features and the 5 day returns for our target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    'Mean_Reversion_Sector_Neutral_Smoothed', 'Momentum_1YR',\n",
    "    'Overnight_Sentiment_Smoothed', 'adv_120d', 'adv_20d',\n",
    "    'dispersion_120d', 'dispersion_20d', 'market_vol_120d',\n",
    "    'market_vol_20d', 'volatility_20d',\n",
    "    'is_Janaury', 'is_December', 'weekday',\n",
    "    'month_end', 'month_start', 'qtr_end', 'qtr_start'] + sector_columns\n",
    "target_label = 'target'\n",
    "\n",
    "temp = all_factors.dropna().copy()\n",
    "X = temp[features]\n",
    "y = temp[target_label]\n",
    "\n",
    "X_train, X_valid, X_test, y_train, y_valid, y_test = train_valid_test_split(X, y, 0.6, 0.2, 0.2)\n",
    "\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests\n",
    "### Visualize a Simple Tree\n",
    "Let's see how a single tree would look using our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "# This is to get consistent results between each run.\n",
    "clf_random_state = 0\n",
    "\n",
    "simple_clf = DecisionTreeClassifier(\n",
    "    max_depth=3,\n",
    "    criterion='entropy',\n",
    "    random_state=clf_random_state)\n",
    "simple_clf.fit(X_train, y_train)\n",
    "\n",
    "display(project_helper.plot_tree_classifier(simple_clf, feature_names=features))\n",
    "project_helper.rank_features_by_importance(simple_clf.feature_importances_, features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: Why does dispersion_20d have the highest feature importance, when the first split is on the Momentum_1YR feature?\n",
    "*#TODO: Put Answer In this Cell*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Random Forests with Different Tree Sizes\n",
    "Let's build models using different tree sizes to find the model that best generalizes.\n",
    "#### Parameters\n",
    "When building the models, we'll use the following parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_days = 10\n",
    "n_stocks = 500\n",
    "\n",
    "clf_parameters = {\n",
    "    'criterion': 'entropy',\n",
    "    'min_samples_leaf': n_stocks * n_days,\n",
    "    'oob_score': True,\n",
    "    'n_jobs': -1,\n",
    "    'random_state': clf_random_state}\n",
    "n_trees_l = [50, 100, 250, 500, 1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall from the lesson, that we’ll choose a min_samples_leaf parameter to be small enough to allow the tree to fit the data with as much detail as possible, but not so much that it overfits.  We can first propose 500, which is the number of assets in the estimation universe. Since we have about 500 stocks in the stock universe, we’ll want at least 500 stocks in a leaf for the leaf to make a prediction that is representative.  It’s common to multiply this by 2,3,5 or 10, so we’d have min samples leaf of 500, 1000, 1500, 2500, and 5000. If we were to try these values, we’d notice that the model is “too good to be true” on the training data.  A good rule of thumb for what is considered “too good to be true”, and therefore a sign of overfitting, is if the sharpe ratio is greater than 4.  Based on this, we recommend using min_sampes_leaf of 10 * 500, or 5,000.\n",
    "\n",
    "Feel free to try other values for these parameters, but also keep in mind that making too many small adjustments to hyper-parameters can lead to overfitting even the validation data, and therefore lead to less generalizable performance on the out-of-sample test set.  So when trying different parameter values, choose values that are different enough in scale (i.e. 10, 20, 100 instead of 10,11,12)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "train_score = []\n",
    "valid_score = []\n",
    "oob_score = []\n",
    "feature_importances = []\n",
    "\n",
    "for n_trees in tqdm(n_trees_l, desc='Training Models', unit='Model'):\n",
    "    clf = RandomForestClassifier(n_trees, **clf_parameters)\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    train_score.append(clf.score(X_train, y_train.values))\n",
    "    valid_score.append(clf.score(X_valid, y_valid.values))\n",
    "    oob_score.append(clf.oob_score_)\n",
    "    feature_importances.append(clf.feature_importances_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the accuracy of the classifiers over the number of trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_helper.plot(\n",
    "    [n_trees_l]*3,\n",
    "    [train_score, valid_score, oob_score],\n",
    "    ['train', 'validation', 'oob'],\n",
    "    'Random Forrest Accuracy',\n",
    "    'Number of Trees')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question: 1) What do you observe with the accuracy vs tree size graph? 2) Does the graph indicate the model is overfitting or underfitting? Describe how it indicates this.\n",
    "*#TODO: Put Answer In this Cell*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's looks at the average feature importance of the classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Features Ranked by Average Importance:\\n')\n",
    "project_helper.rank_features_by_importance(np.average(feature_importances, axis=0), features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might notice that some of the features of low to no importance. We will be removing them when training the final model.\n",
    "### Model Results\n",
    "Let's look at some additional metrics to see how well a model performs. We've created the function `show_sample_results` to show the following results of a model:\n",
    "- Sharpe Ratios\n",
    "- Factor Returns\n",
    "- Factor Rank Autocorrelation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import alphalens as al\n",
    "\n",
    "\n",
    "all_assets = all_factors.index.levels[1].values.tolist()\n",
    "all_pricing = get_pricing(\n",
    "    data_portal,\n",
    "    trading_calendar,\n",
    "    all_assets,\n",
    "    factor_start_date,\n",
    "    universe_end_date)\n",
    "\n",
    "def show_sample_results(data, samples, classifier, factors, pricing=all_pricing):\n",
    "    # Calculate the Alpha Score\n",
    "    prob_array=[-1,1]\n",
    "    alpha_score = classifier.predict_proba(samples).dot(np.array(prob_array))\n",
    "    \n",
    "    # Add Alpha Score to rest of the factors\n",
    "    alpha_score_label = 'AI_ALPHA'\n",
    "    factors_with_alpha = data.loc[samples.index].copy()\n",
    "    factors_with_alpha[alpha_score_label] = alpha_score\n",
    "    \n",
    "    # Setup data for AlphaLens\n",
    "    print('Cleaning Data...\\n')\n",
    "    factor_data = project_helper.build_factor_data(factors_with_alpha[factors + [alpha_score_label]], pricing)\n",
    "    print('\\n-----------------------\\n')\n",
    "    \n",
    "    # Calculate Factor Returns and Sharpe Ratio\n",
    "    factor_returns = project_helper.get_factor_returns(factor_data)\n",
    "    sharpe_ratio = project_helper.sharpe_ratio(factor_returns)\n",
    "    \n",
    "    # Show Results\n",
    "    print('             Sharpe Ratios')\n",
    "    print(sharpe_ratio.round(2))\n",
    "    project_helper.plot_factor_returns(factor_returns)\n",
    "    project_helper.plot_factor_rank_autocorrelation(factor_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results\n",
    "Let's compare our AI Alpha factor to a few other factors. We'll use the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factor_names = [\n",
    "    'Mean_Reversion_Sector_Neutral_Smoothed',\n",
    "    'Momentum_1YR',\n",
    "    'Overnight_Sentiment_Smoothed',\n",
    "    'adv_120d',\n",
    "    'volatility_20d']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training Prediction\n",
    "Let's see how well the model runs on training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_sample_results(all_factors, X_train, clf, factor_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Validation Prediction\n",
    "Let's see how well the model runs on validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_sample_results(all_factors, X_valid, clf, factor_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So that's pretty extraordinary. Even when the input factor returns are sideways to down, the AI Alpha is positive with Sharpe Ratio > 2. If we hope that this model will perform well in production we need to correct though for the non-IID labels and mitigate likely overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overlapping Samples\n",
    "Let's fix this by removing overlapping samples. We can do a number of things:\n",
    "\n",
    "- Don't use overlapping samples\n",
    "- Use BaggingClassifier's `max_samples`\n",
    "- Build an ensemble of non-overlapping trees\n",
    "\n",
    "In this project, we'll do all three methods and compare.\n",
    "### Drop Overlapping Samples\n",
    "This is the simplest of the three methods. We'll just drop any overlapping samples from the dataset. Implement the `non_overlapping_samples` function to return a new dataset overlapping samples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_overlapping_samples(x, y, n_skip_samples, start_i=0):\n",
    "    \"\"\"\n",
    "    Get the non overlapping samples.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : DataFrame\n",
    "        The input samples\n",
    "    y : Pandas Series\n",
    "        The target values\n",
    "    n_skip_samples : int\n",
    "        The number of samples to skip\n",
    "    start_i : int\n",
    "        The starting index to use for the data\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    non_overlapping_x : 2 dimensional Ndarray\n",
    "        The non overlapping input samples\n",
    "    non_overlapping_y : 1 dimensional Ndarray\n",
    "        The non overlapping target values\n",
    "    \"\"\"\n",
    "    assert len(x.shape) == 2\n",
    "    assert len(y.shape) == 1\n",
    "    \n",
    "    # TODO: Implement\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "project_tests.test_non_overlapping_samples(non_overlapping_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the dataset created without overlapping samples, lets train a new model and look at the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_score = []\n",
    "valid_score = []\n",
    "oob_score = []\n",
    "\n",
    "for n_trees in tqdm(n_trees_l, desc='Training Models', unit='Model'):\n",
    "    clf = RandomForestClassifier(n_trees, **clf_parameters)\n",
    "    clf.fit(*non_overlapping_samples(X_train, y_train, 4))\n",
    "    \n",
    "    train_score.append(clf.score(X_train, y_train.values))\n",
    "    valid_score.append(clf.score(X_valid, y_valid.values))\n",
    "    oob_score.append(clf.oob_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_helper.plot(\n",
    "    [n_trees_l]*3,\n",
    "    [train_score, valid_score, oob_score],\n",
    "    ['train', 'validation', 'oob'],\n",
    "    'Random Forrest Accuracy',\n",
    "    'Number of Trees')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_sample_results(all_factors, X_valid, clf, factor_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks better, but we are throwing away a lot of information by taking every 5th row."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use BaggingClassifier's `max_samples`\n",
    "In this method, we'll set `max_samples` to be on the order of the average uniqueness of the labels. Since  `RandomForrestClassifier` does not take this param, we're using `BaggingClassifier`. Implement `bagging_classifier` to build the bagging classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "def bagging_classifier(n_estimators, max_samples, max_features, parameters):\n",
    "    \"\"\"\n",
    "    Build the bagging classifier.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_estimators : int \n",
    "        The number of base estimators in the ensemble\n",
    "    max_samples : float \n",
    "        The proportion of input samples drawn from when training each base estimator\n",
    "    max_features : float \n",
    "        The proportion of input sample features drawn from when training each base estimator\n",
    "    parameters : dict\n",
    "        Parameters to use in building the bagging classifier\n",
    "        It should contain the following parameters:\n",
    "            criterion\n",
    "            min_samples_leaf\n",
    "            oob_score\n",
    "            n_jobs\n",
    "            random_state\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    bagging_clf : Scikit-Learn BaggingClassifier\n",
    "        The bagging classifier\n",
    "    \"\"\"\n",
    "    \n",
    "    required_parameters = {'criterion', 'min_samples_leaf', 'oob_score', 'n_jobs', 'random_state'}\n",
    "    assert not required_parameters - set(parameters.keys())\n",
    "    \n",
    "    # TODO: Implement\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "project_tests.test_bagging_classifier(bagging_classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the bagging classifier built, lets train a new model and look at the results.\n",
    "#### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_score = []\n",
    "valid_score = []\n",
    "oob_score = []\n",
    "\n",
    "for n_trees in tqdm(n_trees_l, desc='Training Models', unit='Model'):\n",
    "    clf = bagging_classifier(n_trees, 0.2, 1.0, clf_parameters)\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    train_score.append(clf.score(X_train, y_train.values))\n",
    "    valid_score.append(clf.score(X_valid, y_valid.values))\n",
    "    oob_score.append(clf.oob_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_helper.plot(\n",
    "    [n_trees_l]*3,\n",
    "    [train_score, valid_score, oob_score],\n",
    "    ['train', 'validation', 'oob'],\n",
    "    'Random Forrest Accuracy',\n",
    "    'Number of Trees')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_sample_results(all_factors, X_valid, clf, factor_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems much \"better\" in the sense that we have much better fidelity between the three.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build an ensemble of non-overlapping trees\n",
    "The last method is to create ensemble of non-overlapping trees. Here we are going to write a custom `scikit-learn` estimator. We inherit from `VotingClassifier` and we override the `fit` method so we fit on non-overlapping periods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import abc\n",
    "\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.base import clone\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import Bunch\n",
    "\n",
    "\n",
    "class NoOverlapVoterAbstract(VotingClassifier):\n",
    "    @abc.abstractmethod\n",
    "    def _calculate_oob_score(self, classifiers):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    @abc.abstractmethod\n",
    "    def _non_overlapping_estimators(self, x, y, classifiers, n_skip_samples):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def __init__(self, estimator, voting='soft', n_skip_samples=4):\n",
    "        # List of estimators for all the subsets of data\n",
    "        estimators = [('clf'+str(i), estimator) for i in range(n_skip_samples + 1)]\n",
    "        \n",
    "        self.n_skip_samples = n_skip_samples\n",
    "        super().__init__(estimators, voting)\n",
    "    \n",
    "    def fit(self, X, y, sample_weight=None):\n",
    "        estimator_names, clfs = zip(*self.estimators)\n",
    "        self.le_ = LabelEncoder().fit(y)\n",
    "        self.classes_ = self.le_.classes_\n",
    "        \n",
    "        clone_clfs = [clone(clf) for clf in clfs]\n",
    "        self.estimators_ = self._non_overlapping_estimators(X, y, clone_clfs, self.n_skip_samples)\n",
    "        self.named_estimators_ = Bunch(**dict(zip(estimator_names, self.estimators_)))\n",
    "        self.oob_score_ = self._calculate_oob_score(self.estimators_)\n",
    "        \n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might notice that two of the functions are abstracted. These will be the functions that you need to implement.\n",
    "#### OOB Score\n",
    "In order to get the correct OOB score, we need to take the average of all the estimator's OOB scores. Implement `calculate_oob_score` to calculate this score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_oob_score(classifiers):\n",
    "    \"\"\"\n",
    "    Calculate the mean out-of-bag score from the classifiers.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    classifiers : list of Scikit-Learn Classifiers\n",
    "        The classifiers used to calculate the mean out-of-bag score\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    oob_score : float\n",
    "        The mean out-of-bag score\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: Implement\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "project_tests.test_calculate_oob_score(calculate_oob_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Non Overlapping Estimators\n",
    "With `calculate_oob_score` implemented, let's create non overlapping estimators. Implement `non_overlapping_estimators` to build non overlapping subsets of the data, then run a estimator on each subset of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_overlapping_estimators(x, y, classifiers, n_skip_samples):\n",
    "    \"\"\"\n",
    "    Fit the classifiers to non overlapping data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : DataFrame\n",
    "        The input samples\n",
    "    y : Pandas Series\n",
    "        The target values\n",
    "    classifiers : list of Scikit-Learn Classifiers\n",
    "        The classifiers used to fit on the non overlapping data\n",
    "    n_skip_samples : int\n",
    "        The number of samples to skip\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    fit_classifiers : list of Scikit-Learn Classifiers\n",
    "        The classifiers fit to the the non overlapping data\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: Implement\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "project_tests.test_non_overlapping_estimators(non_overlapping_estimators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoOverlapVoter(NoOverlapVoterAbstract):\n",
    "    def _calculate_oob_score(self, classifiers):\n",
    "        return calculate_oob_score(classifiers)\n",
    "        \n",
    "    def _non_overlapping_estimators(self, x, y, classifiers, n_skip_samples):\n",
    "        return non_overlapping_estimators(x, y, classifiers, n_skip_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our `NoOverlapVoter` class, let's train it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_score = []\n",
    "valid_score = []\n",
    "oob_score = []\n",
    "\n",
    "for n_trees in tqdm(n_trees_l, desc='Training Models', unit='Model'):\n",
    "    clf = RandomForestClassifier(n_trees, **clf_parameters)\n",
    "    \n",
    "    clf_nov = NoOverlapVoter(clf)\n",
    "    clf_nov.fit(X_train, y_train)\n",
    "    \n",
    "    train_score.append(clf_nov.score(X_train, y_train.values))\n",
    "    valid_score.append(clf_nov.score(X_valid, y_valid.values))\n",
    "    oob_score.append(clf_nov.oob_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_helper.plot(\n",
    "    [n_trees_l]*3,\n",
    "    [train_score, valid_score, oob_score],\n",
    "    ['train', 'validation', 'oob'],\n",
    "    'Random Forrest Accuracy',\n",
    "    'Number of Trees')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_sample_results(all_factors, X_valid, clf_nov, factor_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Model\n",
    "### Re-Training Model\n",
    "In production, we would roll forward the training. Typically you would re-train up to the \"current day\" and then test. Here, we will train on the train & validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_trees = 500\n",
    "\n",
    "clf = RandomForestClassifier(n_trees, **clf_parameters)\n",
    "clf_nov = NoOverlapVoter(clf)\n",
    "clf_nov.fit(\n",
    "    pd.concat([X_train, X_valid]),\n",
    "    pd.concat([y_train, y_valid]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "#### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('train: {}, oob: {}, valid: {}'.format(\n",
    "    clf_nov.score(X_train, y_train.values),\n",
    "    clf_nov.score(X_valid, y_valid.values),\n",
    "    clf_nov.oob_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_sample_results(all_factors, X_train, clf_nov, factor_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_sample_results(all_factors, X_valid, clf_nov, factor_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_sample_results(all_factors, X_test, clf_nov, factor_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, hopefully you are appropriately amazed by this. Despite the significant differences between the factor performances in the three sets, the AI APLHA is able to deliver positive performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission\n",
    "Now that you're done with the project, it's time to submit it. Click the submit button in the bottom right. One of our reviewers will give you feedback on your project with a pass or not passed grade. You can continue to the next section while you wait for feedback."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
